{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Анализ тональности текста с помощью Python\n",
    "\n",
    "\n",
    "Данная статья представляет собой незначительно сокращенный (и пока что частичный) перевод статьи [Кайла Стратиса](https://realpython.com/sentiment-analysis-python/).\n",
    "\n",
    "---\n",
    "\n",
    "[Анализ тональности](https://ru.wikipedia.org/wiki/%D0%90%D0%BD%D0%B0%D0%BB%D0%B8%D0%B7_%D1%82%D0%BE%D0%BD%D0%B0%D0%BB%D1%8C%D0%BD%D0%BE%D1%81%D1%82%D0%B8_%D1%82%D0%B5%D0%BA%D1%81%D1%82%D0%B0) (сентимент-анализ) – инструмент компьютерной лингвистики, оценивающий такую субъективную составляющую текста, как отношение пишущего. Часто это составляет трудность даже для опытных читателей — что уж говорить о программах. Однако эмоциональную окраску текста все-таки можно проанализировать с помощью инструментов экосистемы Python.\n",
    "\n",
    "Зачем это может быть нужно? Существует множество применений для анализа тональности. Например, такие данные позволяют предсказать поведение биржевых трейдеров относительно конкретной компании по откликам в социальных сетях и другим отзывам.\n",
    "\n",
    "В этом руководстве мы рассмотрим:\n",
    "- как использовать методы [обработки естественного языка](https://ru.wikipedia.org/wiki/%D0%9E%D0%B1%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%BA%D0%B0_%D0%B5%D1%81%D1%82%D0%B5%D1%81%D1%82%D0%B2%D0%B5%D0%BD%D0%BD%D0%BE%D0%B3%D0%BE_%D1%8F%D0%B7%D1%8B%D0%BA%D0%B0) (англ. natural language processing, NLP);\n",
    "- как использовать машинное обучение для определения тональности текста;\n",
    "- как использовать библиотеку spaCy для создания классификатора анализа настроений.\n",
    "\n",
    "Это руководство предназначено для начинающих практиков машинного обучения.\n",
    "\n",
    "\n",
    "# Предварительная обработка и очистка текстовых данных\n",
    "\n",
    "Любой рабочий процесс анализа данных начинается с их загрузки. Но после того, как данные будут загружены, мы должны будем пропустить их через своеобразный конвейер (pipeline) обработки естественного языка.\n",
    "\n",
    "В частности, нам потребуется сделать следующие шаги:\n",
    "- **токенизировать текст** – разбить текст на предложения, слова и другие единицы;\n",
    "- **удалить стоп-слова** – «если», «но», «или» и т. д.;\n",
    "- **привести слова к нормальной форме**;\n",
    "- **привести текст к векторной форме** – сделать числовое представление текста для его дальнейшей обработки классификатором.\n",
    "\n",
    "Все эти шаги служат для уменьшения шума, присущего любому обычному тексту, и повышения точности результатов классификатора. Эти задачи решают множество отличных библиотек – [NLTK](https://www.nltk.org/), [TextBlob](https://textblob.readthedocs.io/en/dev/index.html) и [spaCy](https://spacy.io/) – последнюю мы и будем применять в этом руководстве мы будем использовать spaCy.\n",
    "\n",
    "Прежде чем идти дальше, убедитесь, что у вас установлен spaCy и модель для английского язка (расскомментриуйте две следующие строки, чтобы установить библиотеку и модуль):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Примечание переводчика**. Об использовании spaCy для русского языка подробно написано в README-файле репозитория [spaCy](https://github.com/buriy/spacy-ru).\n",
    "\n",
    "---\n",
    "\n",
    "# Токенизация\n",
    "\n",
    "Токенизация – это процесс разбиения текста на более мелкие части. Библиотека spaCy поставляется вместе с конвейером, начинающем обработку текста с токенизации. Можно выполнять токенизацию предложений и слов. В этом руководстве мы разделим текст на отдельные слова. Загрузим текст примера и проведем разбиение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", Dave, watched, as, the, forest, burned, up, on, the, hill, ,, \n",
      ", only, a, few, miles, from, his, house, ., The, car, had, \n",
      ", been, hastily, packed, and, Marta, was, inside, trying, to, round, \n",
      ", up, the, last, of, the, pets, ., \", Where, could, she, be, ?, \", he, wondered, \n",
      ", as, he, continued, to, wait, for, Marta, to, appear, with, the, pets, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "text = \"\"\"\n",
    "Dave watched as the forest burned up on the hill,\n",
    "only a few miles from his house. The car had\n",
    "been hastily packed and Marta was inside trying to round\n",
    "up the last of the pets. \"Where could she be?\" he wondered\n",
    "as he continued to wait for Marta to appear with the pets.\n",
    "\"\"\"\n",
    "\n",
    "doc = nlp(text)\n",
    "token_list = [token for token in doc]\n",
    "\n",
    "print(token_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вы могли заметить, что захваченные токены включают знаки препинания и другие строки, не относящиеся к словам. Это нормальное поведение в случае использований конвейера по умолчанию.\n",
    "\n",
    "# Удаление стоп-слов\n",
    "\n",
    "Стоп-слова – это слова, которые могут иметь важное значение в человеческом общении, но не имеют большого значения для машин. spaCy поставляется со списком стоп-слов по умолчанию (его можно настроить). Проведем фильтрацию полученного списка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      ", Dave, watched, forest, burned, hill, ,, \n",
      ", miles, house, ., car, \n",
      ", hastily, packed, Marta, inside, trying, round, \n",
      ", pets, ., \", ?, \", wondered, \n",
      ", continued, wait, Marta, appear, pets, ., \n",
      "]\n"
     ]
    }
   ],
   "source": [
    "filtered_tokens = [token for token in doc if not token.is_stop]\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одной строкой Python-кода мы отфильтровали стоп-слова из токенизированного текста с помощью атрибута токена `.is_stop`. После удаления стоп-слов список токенов стал короче, исчезли местоимения и служебные слова: артикли, союзы, предлоги и послелоги.\n",
    "\n",
    "# Приведение к нормальной форме\n",
    "Процесс **нормализации** несколько сложнее токенизации. Все формы слова мы должны привести к единому представлению. Например, `watched`, `watched` и `watches` после нормализации превращаются в `watch`. Есть два основных подхода к нормализации:\n",
    "1. стемминг;\n",
    "2. лемматизация.\n",
    "\n",
    "В случае **стемминга** выделяется основа слова, дополнив которую можно получить слова-потомки. Такой метод сработает на приведенном примере. Однако это слишком простой подход – стемминг просто обрезает строку, отбрасывая окончание. Стемминг не обнаружит связь между `feel` и `felt`.\n",
    "\n",
    "**Лемматизация** стремится решить указанную проблему, используя структуру данных, в которой все формы слова связаны с его простейшей формой – леммой. Лемматизация обычно приносит больше пользы, чем стемминг, и потому является единственной стратегия нормализации, предлагаемая spaCy.\n",
    "\n",
    "В рамках NLP-конвейера лемметизация происходит автоматически (вместе с рядом других действий). Лемма для каждого токена хранится в атрибуте `.lemma_`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Token: \\n, lemma: \\n', 'Token: Dave, lemma: Dave', 'Token: watched, lemma: watch', 'Token: forest, lemma: forest', 'Token: burned, lemma: burn', 'Token: hill, lemma: hill', 'Token: ,, lemma: ,', 'Token: \\n, lemma: \\n', 'Token: miles, lemma: mile', 'Token: house, lemma: house', 'Token: ., lemma: .', 'Token: car, lemma: car', 'Token: \\n, lemma: \\n', 'Token: hastily, lemma: hastily', 'Token: packed, lemma: pack', 'Token: Marta, lemma: Marta', 'Token: inside, lemma: inside', 'Token: trying, lemma: try', 'Token: round, lemma: round', 'Token: \\n, lemma: \\n', 'Token: pets, lemma: pet', 'Token: ., lemma: .', 'Token: \", lemma: \"', 'Token: ?, lemma: ?', 'Token: \", lemma: \"', 'Token: wondered, lemma: wonder', 'Token: \\n, lemma: \\n', 'Token: continued, lemma: continue', 'Token: wait, lemma: wait', 'Token: Marta, lemma: Marta', 'Token: appear, lemma: appear', 'Token: pets, lemma: pet', 'Token: ., lemma: .', 'Token: \\n, lemma: \\n']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [\n",
    "    f\"Token: {token}, lemma: {token.lemma_}\"\n",
    "    for token in filtered_tokens\n",
    "]\n",
    "\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Примечание**. Обратите внимание на символ подчеркивания в атрибуте `.lemma_`. Это не опечатка, а результат соглашения по именованию в spaCy   версий атрибутов, которые могут быть прочитатаны человеком.\n",
    "\n",
    "---\n",
    "\n",
    "Следующим шагом является представление каждого токена способом, понятным машине. Этот процесс называется векторизацией.\n",
    "\n",
    "\n",
    "# Векторизация текста\n",
    "\n",
    "Векторизация – преобразование токена в вектор (числовой массив), который представляет его свойства и в контексте задачи является уникальным для каждого токена. Векторные представления токенов используются в поиске сходства слов, классификации текстов и т. д.\n",
    "\n",
    "В spaCy токены векторизуются в виде плотный массивов, в которых определены значения для каждой позиции массива. Это отличает используемый подход от ранних методов, в которых для тех же целей применялись разреженные массивы, а большинство позиций были заполнены нулями.\n",
    "\n",
    "Как и другие шаги, векторизация выполняется автоматически посредством вызова `nlp()`. Поскольку у нас уже есть список объектов токенов, мы можем получить векторное представление одного из токенов следующим образом:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.8371646 ,  1.4529226 , -1.6147211 ,  0.678362  , -0.6594443 ,\n",
       "        1.6417935 ,  0.5796405 ,  2.3021278 , -0.13260496,  0.5750932 ,\n",
       "        1.5654886 , -0.6938864 , -0.59607106, -1.5377437 ,  1.9425622 ,\n",
       "       -2.4552505 ,  1.2321601 ,  1.0434952 , -1.5102385 , -0.5787632 ,\n",
       "        0.12055647,  3.6501784 ,  2.6160972 , -0.5710199 , -1.5221789 ,\n",
       "        0.00629176,  0.22760668, -1.922073  , -1.6252862 , -4.226225  ,\n",
       "       -3.495663  , -3.312053  ,  0.81387717, -0.00677544, -0.11603224,\n",
       "        1.4620426 ,  3.0751472 ,  0.35958546, -0.22527039, -2.743926  ,\n",
       "        1.269633  ,  4.606786  ,  0.34034157, -2.1272311 ,  1.2619178 ,\n",
       "       -4.209798  ,  5.452852  ,  1.6940253 , -2.5972986 ,  0.95049495,\n",
       "       -1.910578  , -2.374927  , -1.4227567 , -2.2528825 , -1.799806  ,\n",
       "        1.607501  ,  2.9914255 ,  2.8065152 , -1.2510269 , -0.54964066,\n",
       "       -0.49980402, -1.3882618 , -0.470479  , -2.9670253 ,  1.7884955 ,\n",
       "        4.5282774 , -1.2602427 , -0.14885521,  1.0419178 , -0.08892632,\n",
       "       -1.138275  ,  2.242618  ,  1.5077229 , -1.5030195 ,  2.528098  ,\n",
       "       -1.6761329 ,  0.16694719,  2.123961  ,  0.02546412,  0.38754445,\n",
       "        0.8911977 , -0.07678384, -2.0690763 , -1.1211847 ,  1.4821006 ,\n",
       "        1.1989193 ,  2.1933236 ,  0.5296372 ,  3.0646474 , -1.7223308 ,\n",
       "       -1.3634219 , -0.47471118, -1.7648507 ,  3.565178  , -2.394205  ,\n",
       "       -1.3800384 ], dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_tokens[1].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь мы используем атрибут `.vector` для второго токена в списке `filter_tokens`. В этом наборе это слово `Dave`.\n",
    "\n",
    "---\n",
    "\n",
    "**Примечание**. Если для атрибута `.vector` вы получите другой результат, не беспокойтесь. Это может быть связано с тем, что используется другая версия модели `en_core_web_sm` или самой библиотеки `spaCy`.\n",
    "\n",
    "---\n",
    "\n",
    "Теперь, когда вы узнали о некоторых типичных этапах предварительной обработки текста в spaCy, узнаем, как классифицировать текст.\n",
    "\n",
    "\n",
    "# Использование классификаторов машинного обучения для прогнозирования настроений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
