{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решение задач из курса [Нейронные сети и обработка текста](https://stepik.org/course/54098).\n",
    "\n",
    "# Задача поиска\n",
    "Процесс решения задачи поиска часто состоит из двух крупных шагов, каждый из которых разбивается на более мелкие:\n",
    "\n",
    "1. Настройка поиска\n",
    "    1. Преобразование объектов (например, текстов) в вещественные вектора\n",
    "    2. Построение поискового индекса\n",
    "    3. Настройка функции ранжирования\n",
    "2. Выполнение поиска\n",
    "    1. Преобразование запроса в вещественный вектор\n",
    "    2. Грубая выборка кандидатов\n",
    "    3. Сртировка кандидатов с помощью функции ранжирования\n",
    "\n",
    "Поисковый индекс – набор специальных структур данных, ускоряющих процесс поиска. Так как процесс индексации тоже требует времени, поисковые индексы не всегда имеет смысл строить – например, когда данных мало или данные часто меняются и индекс устаревает быстрее, чем может быть перестроен.\n",
    "\n",
    "Настройка функции ранжирования выполняется с помощью набора примеров вида \"запрос – документ – оценка релевантности по мнению человека\". Релевантность – численная величина, характеризующая соответствие найденного документа запросу (чем больше, тем лучше документ подходит под запрос).\n",
    "\n",
    "Пусть у нас есть коллекция документов, которые на шаге 1.1 были преобразованы в следующие вектора\n",
    "\n",
    "ID документа | Признаки\n",
    "--- | ---\n",
    "1 | (0, 1)\n",
    "2 | (1, 0)\n",
    "3 | (1, 0.5)\n",
    "\n",
    "Также у нас есть функция ранжирования $Relevance(q, d) = -(q_1 - d_1)^2 - 2 (q_2 - d_2)^2$, где $(q_1, q_2)$ – признаки запроса, а $(d_1, d_2)$ признаки документа.\n",
    "\n",
    "Нам пришел запрос с признаками (1, 1).\n",
    "\n",
    "Определите релевантность документов относительно данного запроса."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance for (0.0, 1.0) is -1.0.\n",
      "Relevance for (1.0, 0.0) is -2.0.\n",
      "Relevance for (1.0, 0.5) is -0.5.\n"
     ]
    }
   ],
   "source": [
    "def relevance(q, d):\n",
    "    return -(q[0] - d[0])**2 - 2*((q[1] - d[1])**2)\n",
    "\n",
    "docs = [(0.0, 1.0),\n",
    "        (1.0, 0.0),\n",
    "        (1.0, 0.5)]\n",
    "\n",
    "q = (1, 1)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"Relevance for {doc} is {relevance(q, doc)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение логистической регрессии. Градиентный спуск\n",
    "\n",
    "**Задача**. Вы обучаете одномерную логистическую регрессию $\\hat{y}(x) = \\frac{1} {1 + e^{-wx - b}}$ , то есть $w \\in R$ – это скаляр (число), $x$ –  единственный признак входного объекта, $y(x) \\in \\{0, 1\\}$ – настоящий класс объекта $x$, $ 0 < \\hat{y}(x) < 1$ – предсказанная вероятность того, что $x$ принадлежит к классу 1.\n",
    "\n",
    "В качестве функции потерь вы используете бинарную кросс-энтропию $$BCE(\\hat{y}, y) = -y \\log\\hat{y} - (1 - y) \\log(1 - \\hat{y})$$\n",
    "\n",
    "Найдите в общем виде производную функции потерь по $w$ $\\frac{\\partial BCE(\\hat{y}, y)}{\\partial w}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{x \\left(- y e^{- b - w x} - y + 1\\right)}{e^{- b - w x} + 1}$"
      ],
      "text/plain": [
       "x*(-y*exp(-b - w*x) - y + 1)/(exp(-b - w*x) + 1)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sympy import *\n",
    "x, y, w, b = symbols('x y w b')\n",
    "y_hat = 1/(1+exp(-w*x-b))\n",
    "eq = -y*log(y_hat)-(1-y)*log(1-y_hat)\n",
    "diff(eq, w).simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдите в общем виде производную функции потерь по $b$ $\\frac{\\partial BCE(\\hat{y}, y)}{\\partial b}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{- y e^{- b - w x} - y + 1}{e^{- b - w x} + 1}$"
      ],
      "text/plain": [
       "(-y*exp(-b - w*x) - y + 1)/(exp(-b - w*x) + 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff(eq, b).simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте L2-регуляризацию $Loss(\\hat{y}, y) = BCE(\\hat{y}, y) + c(w^2 + b^2)$, где $c > 0$ – коэффициент регуляризации. Расчитайте производную функции потерь по w."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\frac{2 c w \\left(e^{- b - w x} + 1\\right) - x y e^{- b - w x} - x \\left(y - 1\\right)}{e^{- b - w x} + 1}$"
      ],
      "text/plain": [
       "(2*c*w*(exp(-b - w*x) + 1) - x*y*exp(-b - w*x) - x*(y - 1))/(exp(-b - w*x) + 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = symbols('c')\n",
    "eq += c*(w*w + b*b)\n",
    "diff(eq, w).simplify()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя формулу производной с предыдущего шага, запишите формулу для обновления веса $w$ с помощью стохастического градиентного спуска (размер минибатча равен 1). Для обозначения скорости обучения (learning rate) используйте маленькую латинскую букву $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{t \\left(2 c w \\left(e^{- b - w x} + 1\\right) - x y e^{- b - w x} - x \\left(y - 1\\right)\\right)}{e^{- b - w x} + 1} + w$"
      ],
      "text/plain": [
       "-t*(2*c*w*(exp(-b - w*x) + 1) - x*y*exp(-b - w*x) - x*(y - 1))/(exp(-b - w*x) + 1) + w"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = symbols('t')\n",
    "w_new = w - t*diff(eq, w).simplify()\n",
    "w_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Точечная взаимная информация двух случайных событий\n",
    "\n",
    "**Задача**. Напишите функцию для вычисления точечной взаимной информации двух случайных событий.\n",
    "\n",
    "$pmi(a, b) = \\ln \\dfrac{p(a, b)} {p(a) p(b)}$\n",
    "\n",
    "На вход функция получает два массива из 0 и 1 одинаковой длины – реализации случайных событий: 1 – событие произошло, 0 – не произошло. В результате функция должна вернуть вещественное число – точечную взаимную информацию событий.\n",
    "\n",
    "```\n",
    "Sample Input:\n",
    "    1 0 0 1 1 0\n",
    "    1 0 0 0 1 0\n",
    "\n",
    "Sample Output:\n",
    "    0.693147\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.693147\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import Counter\n",
    " \n",
    "def calculate_pmi(a, b):\n",
    "    def prob(arr):\n",
    "        return Counter(arr)[1]/len(arr)\n",
    "\n",
    "    p_ab = Counter(zip(a, b))[(1, 1)]/len(a)\n",
    "    return np.log(p_ab/(prob(a)*prob(b)))\n",
    "\n",
    "a = [1, 0, 0, 1, 1, 0]\n",
    "b = [1, 0, 0, 0, 1, 0]\n",
    "pmi_value = calculate_pmi(a, b)\n",
    "\n",
    "print('{:.6f}'.format(pmi_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**. Найдите количество слов, которые встречаются менее, чем в 10 из 10000 документов, если предполагать, что вероятность встретить слово в документе распределена по Ципфу с параметром $s = 2$, количество слов в словаре $N = 1000$. Ранги нумеруются с 1.\n",
    "\n",
    "**Решение**. Плотность распределения Ципфа имеет следующий вид:\n",
    "\n",
    "$$f(rank,s, N) = \\dfrac{1}{Z(s, N)rank^s},$$\n",
    "\n",
    "где $rank$ – порядковый номер слова после сортировки по убыванию частоты, $s$ – коэффициент скорости убывания вероятности, $N$ – количество слов, $Z(s, N) = \\sum_{i=1}^N i^{-s}$ – нормализационная константа."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Значение Z(s, N) для s = 2 и N = 1000 составляет 1.64.\n"
     ]
    }
   ],
   "source": [
    "s = 2\n",
    "N = 1000\n",
    "\n",
    "def Z(s, N):\n",
    "    return sum(i**-s for i in range(1, N+1))\n",
    "\n",
    "print(f'Значение Z(s, N) для s = {s} и N = {N} составляет {Z(s, N):.2f}.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем заданный в условии критерий отбора: $f_c < 0.001$ (т.е. 10/10000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976\n"
     ]
    }
   ],
   "source": [
    "Z_selected = Z(s, N)\n",
    "f_c = 0.001\n",
    "\n",
    "for rank in range(1, 1000):\n",
    "    f = 1/(Z_selected*rank**s)\n",
    "    if f < f_c:\n",
    "        break\n",
    "\n",
    "# нам нужно предыдущее значение ранга\n",
    "# перед тем как оно стало ниже критического\n",
    "rank -= 1\n",
    "print(N - rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача**. Дана следующая коллекция текстов. Постройте словарь (отображение из строкового представления токенов в их номера) и вектор весов (DF).\n",
    "\n",
    "\n",
    "```\n",
    "Казнить нельзя, помиловать. Нельзя наказывать.\n",
    "\n",
    "Казнить, нельзя помиловать. Нельзя освободить.\n",
    "\n",
    "Нельзя не помиловать.\n",
    "\n",
    "Обязательно освободить.\n",
    "\n",
    "```\n",
    "\n",
    "$DF(w) = \\frac{DocCount(w, c)}{Size(c)}$ – частота слова $w$ в коллекции $c$ (отношение количества документов, в которых слово используется, к общему количеству документов)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "наказывать не обязательно казнить освободить нельзя помиловать\n",
      "0.25 0.25 0.25 0.50 0.50 0.75 0.75\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "list_ = ['Казнить нельзя, помиловать. Нельзя наказывать.',\n",
    "         'Казнить, нельзя помиловать. Нельзя освободить.',\n",
    "         'Нельзя не помиловать.',\n",
    "         'Обязательно освободить.']\n",
    "\n",
    "vector = TfidfVectorizer(smooth_idf=False, use_idf=True)\n",
    "vector.fit_transform(list_)\n",
    "freq = 1/np.exp(vectorizer.idf_ - 1)\n",
    "\n",
    "df = pd.DataFrame(zip(vector.get_feature_names(), freq)).sort_values(by=1)\n",
    "print(' '.join(df[0].values))\n",
    "print(' '.join([f'{i:.2f}' for i in df[1].values]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
